{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Grid size and global parameters\n",
    "GRID_SIZE = 12\n",
    "GOAL = (11, 6)\n",
    "START = (0, 6)\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
    "\n",
    "# Stochastic transition probabilities\n",
    "INTENDED_PROB = 0.8\n",
    "PERP_PROB = 0.1\n",
    "\n",
    "# Maze grid (0 = free space, 1 = obstacle)\n",
    "maze_grid = np.array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
    "                      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                      [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                      [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                      [1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1],\n",
    "                      [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                      [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                      [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1],\n",
    "                      [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
    "                      [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "                      [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "# Reward function based on the current and next state\n",
    "def get_reward(state, next_state):\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif maze_grid[next_state[0]][next_state[1]] == 1:  # Hitting an obstacle\n",
    "        return -10\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Check if a state is valid (within bounds and not an obstacle)\n",
    "def is_valid_state(state):\n",
    "    return 0 <= state[0] < GRID_SIZE and 0 <= state[1] < GRID_SIZE and maze_grid[state[0]][state[1]] == 0\n",
    "\n",
    "# Get the next state based on the chosen action and stochastic transitions\n",
    "def get_next_state(state, action):\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if is_valid_state(next_state):\n",
    "        return next_state\n",
    "    else:\n",
    "        return state  # Stay in the same state if the move is invalid\n",
    "\n",
    "# Get the list of possible next states with probabilities\n",
    "def get_possible_transitions(state, action):\n",
    "    transitions = []\n",
    "    intended_next = get_next_state(state, action)\n",
    "    transitions.append((INTENDED_PROB, intended_next))\n",
    "    \n",
    "    # Perpendicular moves\n",
    "    if action == (-1, 0) or action == (1, 0):  # Moving Up or Down\n",
    "        transitions.append((PERP_PROB, get_next_state(state, (0, -1))))  # Left\n",
    "        transitions.append((PERP_PROB, get_next_state(state, (0, 1))))   # Right\n",
    "    else:  # Moving Left or Right\n",
    "        transitions.append((PERP_PROB, get_next_state(state, (-1, 0))))  # Up\n",
    "        transitions.append((PERP_PROB, get_next_state(state, (1, 0))))   # Down\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "# Policy Iteration parameters\n",
    "GAMMA = 0.9\n",
    "THETA = 0.01\n",
    "EPISODES = 1000\n",
    "\n",
    "# Initialize random policy and value function\n",
    "policy = np.random.choice(len(ACTIONS), (GRID_SIZE, GRID_SIZE))  # Random policy\n",
    "value_function = np.zeros((GRID_SIZE, GRID_SIZE))  # Value function\n",
    "q_function = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))  # Q-function\n",
    "\n",
    "# Policy iteration with G_t calculation\n",
    "def policy_iteration():\n",
    "    global policy, value_function, q_function\n",
    "    returns = []  # To store cumulative return G_t for each episode\n",
    "\n",
    "    is_policy_stable = False\n",
    "    while not is_policy_stable:\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for i in range(GRID_SIZE):\n",
    "                for j in range(GRID_SIZE):\n",
    "                    state = (i, j)\n",
    "                    if state == GOAL:\n",
    "                        continue  # Skip goal state\n",
    "                    \n",
    "                    v = value_function[i][j]\n",
    "                    action = ACTIONS[policy[i][j]]\n",
    "                    value_sum = 0\n",
    "                    \n",
    "                    # Calculate expected value considering stochastic transitions\n",
    "                    for prob, next_state in get_possible_transitions(state, action):\n",
    "                        reward = get_reward(state, next_state)\n",
    "                        value_sum += prob * (reward + GAMMA * value_function[next_state[0]][next_state[1]])\n",
    "                    \n",
    "                    value_function[i][j] = value_sum\n",
    "                    delta = max(delta, abs(v - value_function[i][j]))\n",
    "            \n",
    "            if delta < THETA:\n",
    "                break\n",
    "\n",
    "        # Calculate G_t for the current episode\n",
    "        G_t = 0\n",
    "        state = START\n",
    "        episode_rewards = []\n",
    "        \n",
    "        while state != GOAL:\n",
    "            action = ACTIONS[policy[state[0]][state[1]]]\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # Sum the discounted rewards to compute G_t\n",
    "        for k, reward in enumerate(episode_rewards):\n",
    "            G_t += (GAMMA ** k) * reward\n",
    "\n",
    "        returns.append(G_t)\n",
    "\n",
    "        # Policy Improvement\n",
    "        is_policy_stable = True\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL:\n",
    "                    continue  # Skip goal state\n",
    "                \n",
    "                old_action = policy[i][j]\n",
    "                \n",
    "                # Find the action that maximizes expected value with stochastic transitions\n",
    "                action_values = []\n",
    "                for action_idx, action in enumerate(ACTIONS):\n",
    "                    value_sum = 0\n",
    "                    for prob, next_state in get_possible_transitions(state, action):\n",
    "                        reward = get_reward(state, next_state)\n",
    "                        value_sum += prob * (reward + GAMMA * value_function[next_state[0]][next_state[1]])\n",
    "                    action_values.append(value_sum)\n",
    "                    \n",
    "                    # Update q_function for q(s, a)\n",
    "                    q_function[i][j][action_idx] = value_sum\n",
    "                \n",
    "                best_action = np.argmax(action_values)\n",
    "                policy[i][j] = best_action\n",
    "\n",
    "                if old_action != best_action:\n",
    "                    is_policy_stable = False\n",
    "\n",
    "    return policy, value_function, q_function, returns\n",
    "\n",
    "# Run policy iteration\n",
    "optimal_policy, optimal_value_function, optimal_q_function, cumulative_returns = policy_iteration()\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"\\nOptimal Value Function (V*):\")\n",
    "print(optimal_value_function)\n",
    "\n",
    "print(\"\\nOptimal Q Function (Q*(s, a)):\")\n",
    "print(optimal_q_function)\n",
    "\n",
    "print(\"\\nCumulative Returns (G_t) per Episode:\")\n",
    "print(cumulative_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
